Using cuda device
1 train sum loss: 458.146 test sum loss: 82.161 train mean loss: 0.586 test mean loss: 0.523
Best epoch so far: 1
2 train sum loss: 387.230 test sum loss: 74.148 train mean loss: 0.495 test mean loss: 0.472
Best epoch so far: 2
3 train sum loss: 343.399 test sum loss: 68.526 train mean loss: 0.439 test mean loss: 0.436
Best epoch so far: 3
4 train sum loss: 309.847 test sum loss: 62.726 train mean loss: 0.396 test mean loss: 0.400
Best epoch so far: 4
5 train sum loss: 277.283 test sum loss: 54.244 train mean loss: 0.355 test mean loss: 0.346
Best epoch so far: 5
6 train sum loss: 245.282 test sum loss: 58.233 train mean loss: 0.314 test mean loss: 0.371
7 train sum loss: 223.511 test sum loss: 50.896 train mean loss: 0.286 test mean loss: 0.324
Best epoch so far: 7
8 train sum loss: 207.155 test sum loss: 47.083 train mean loss: 0.265 test mean loss: 0.300
Best epoch so far: 8
9 train sum loss: 191.001 test sum loss: 43.980 train mean loss: 0.244 test mean loss: 0.280
Best epoch so far: 9
10 train sum loss: 178.192 test sum loss: 43.661 train mean loss: 0.228 test mean loss: 0.278
Best epoch so far: 10
11 train sum loss: 165.791 test sum loss: 46.116 train mean loss: 0.212 test mean loss: 0.294
12 train sum loss: 156.494 test sum loss: 46.766 train mean loss: 0.200 test mean loss: 0.298
13 train sum loss: 144.976 test sum loss: 45.289 train mean loss: 0.185 test mean loss: 0.288
14 train sum loss: 137.048 test sum loss: 41.282 train mean loss: 0.175 test mean loss: 0.263
Best epoch so far: 14
15 train sum loss: 129.782 test sum loss: 43.494 train mean loss: 0.166 test mean loss: 0.277
16 train sum loss: 122.381 test sum loss: 42.978 train mean loss: 0.156 test mean loss: 0.274
17 train sum loss: 114.129 test sum loss: 42.568 train mean loss: 0.146 test mean loss: 0.271
18 train sum loss: 107.141 test sum loss: 43.733 train mean loss: 0.137 test mean loss: 0.279
19 train sum loss: 102.055 test sum loss: 43.709 train mean loss: 0.131 test mean loss: 0.278
20 train sum loss: 97.123 test sum loss: 44.758 train mean loss: 0.124 test mean loss: 0.285
21 train sum loss: 92.240 test sum loss: 46.594 train mean loss: 0.118 test mean loss: 0.297
22 train sum loss: 87.144 test sum loss: 46.294 train mean loss: 0.111 test mean loss: 0.295
23 train sum loss: 83.272 test sum loss: 44.688 train mean loss: 0.106 test mean loss: 0.285
24 train sum loss: 78.224 test sum loss: 47.431 train mean loss: 0.100 test mean loss: 0.302
25 train sum loss: 75.589 test sum loss: 47.408 train mean loss: 0.097 test mean loss: 0.302
26 train sum loss: 69.695 test sum loss: 48.952 train mean loss: 0.089 test mean loss: 0.312
27 train sum loss: 70.189 test sum loss: 52.366 train mean loss: 0.090 test mean loss: 0.334
28 train sum loss: 63.968 test sum loss: 52.397 train mean loss: 0.082 test mean loss: 0.334
29 train sum loss: 61.654 test sum loss: 53.533 train mean loss: 0.079 test mean loss: 0.341
([458.14586848020554, 387.2295967042446, 343.39886957407, 309.84683659672737, 277.28326196968555, 245.2815291583538, 223.51053147017956, 207.15518499910831, 191.0012536495924, 178.19220102578402, 165.79142605513334, 156.49369290471077, 144.97587399929762, 137.04809376597404, 129.7816135250032, 122.3812342658639, 114.12943714484572, 107.14113454148173, 102.05507248267531, 97.12310152128339, 92.23950847890228, 87.14370862022042, 83.27165134251118, 78.2240364421159, 75.58923303894699, 69.6952263545245, 70.18904124759138, 63.96814228966832, 61.653503857553005], [82.1607112288475, 74.14787596464157, 68.52552106976509, 62.725889295339584, 54.24425768852234, 58.23331119120121, 50.89641983807087, 47.08333384990692, 43.97977439314127, 43.66096729040146, 46.11590322852135, 46.765999883413315, 45.288768976926804, 41.28199175000191, 43.49423451721668, 42.97763350605965, 42.56794339418411, 43.73289495706558, 43.70913079380989, 44.75771343708038, 46.593519784510136, 46.29363372921944, 44.68766202032566, 47.43058793991804, 47.407647363841534, 48.95209600031376, 52.36601596325636, 52.39665785431862, 53.53329297155142], [0.5858642819439969, 0.49517851240952, 0.4391289892251534, 0.3962235762106488, 0.35458217643182294, 0.3136592444480228, 0.28581909395163624, 0.264904328643361, 0.24424712742914628, 0.22786726473885424, 0.21200949623418586, 0.20011981189860711, 0.1853911432216082, 0.17525331683628395, 0.1659611426150936, 0.15649774202795894, 0.1459455717964779, 0.13700912345458022, 0.13050520777835717, 0.1241983395412831, 0.11795333565077018, 0.1114369675450389, 0.10648548765027005, 0.10003073713825562, 0.09666142332346161, 0.08912433037662978, 0.08975580722198385, 0.08180069346504901, 0.07884079777180691], [0.5858642819439969, 0.49517851240952, 0.4391289892251534, 0.3962235762106488, 0.35458217643182294, 0.3136592444480228, 0.28581909395163624, 0.264904328643361, 0.24424712742914628, 0.22786726473885424, 0.21200949623418586, 0.20011981189860711, 0.1853911432216082, 0.17525331683628395, 0.1659611426150936, 0.15649774202795894, 0.1459455717964779, 0.13700912345458022, 0.13050520777835717, 0.1241983395412831, 0.11795333565077018, 0.1114369675450389, 0.10648548765027005, 0.10003073713825562, 0.09666142332346161, 0.08912433037662978, 0.08975580722198385, 0.08180069346504901, 0.07884079777180691])